{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fd9650",
   "metadata": {},
   "source": [
    "![Description of Image](Airline_satisfaction_cover.png)\n",
    "\n",
    "### ‚ùì Problem Statment\n",
    "- Description\n",
    "\n",
    "Analyzing Airline Passenger Satisfaction Data to discover factors affecting satisfaction and to build a predictive model for it.\n",
    "\n",
    "- Objective\n",
    "\n",
    "To predict passenger satisfaction (Satisfied vs. Neutral/Dissatisfied) using machine learning models and to gain insights into the most influential features affecting satisfaction.\n",
    "\n",
    "- Approach\n",
    "\n",
    "    - Load and clean the data\n",
    "\n",
    "    - Handle missing/duplicate/mixed-type values\n",
    "\n",
    "    - Explore the data through visualizations\n",
    "\n",
    "    - Build and evaluate multiple ML models\n",
    "\n",
    "    - Select the best-performing model\n",
    "\n",
    "    - Save it using joblib for later integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c53fe0",
   "metadata": {},
   "source": [
    "# üìäüîß Data Environment Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data manipulation and visualization\n",
    "import pandas as pd                                         # for working with data in tabular form (DataFrames)\n",
    "import numpy as np                                          # for numerical computations and array operations\n",
    "\n",
    "import matplotlib.pyplot as plt                             # for creating plots and charts\n",
    "import seaborn as sns                                       # for advanced visualizations with better styling\n",
    "from colorama import Fore                                   # for printing colored text in the console (e.g., errors, alerts)\n",
    "\n",
    "from sklearn.model_selection import train_test_split        # Importing the train_test_split function to split the dataset into training and testing sets\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder              # Import class to convert categorical labels to numeric codes\n",
    "\n",
    "from sklearn.model_selection import cross_val_score         # Import cross_val_score from sklearn.model_selection to perform cross-validation on the model\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression         # Logistic Regression: a simple and interpretable linear classification model\n",
    "from sklearn.tree import DecisionTreeClassifier             # Decision Tree: a non-linear model that splits data into branches for classification\n",
    "from sklearn.ensemble import RandomForestClassifier         # Random Forest: an ensemble of decision trees to reduce overfitting\n",
    "from sklearn.ensemble import GradientBoostingClassifier     # Gradient Boosting: builds trees sequentially to correct errors from previous ones    \n",
    "from sklearn.svm import SVC                                 # Support Vector Classifier: effective in high-dimensional spaces, aims to find the optimal decision boundary\n",
    "from sklearn.neighbors import KNeighborsClassifier          # K-Nearest Neighbors: classifies based on the majority label of nearest data points\n",
    "from xgboost import XGBClassifier                           # XGBoost: a powerful and efficient gradient boosting algorithm optimized for speed and performance\n",
    "\n",
    "import time                                                 # Importing the time library to calc the time to train models\n",
    "from sklearn.metrics import classification_report           # Importing the classification_report to generate a detailed performance report (precision, recall, and F1-score).\n",
    "from sklearn.metrics import accuracy_score                  # Importing accuracy_score to calculate the accuracy of the classification model\n",
    "from sklearn.metrics import confusion_matrix                # Used to compute the confusion matrix for actual vs predicted labels\n",
    "from sklearn.metrics import ConfusionMatrixDisplay          # Used to visualize the confusion matrix as a plot\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV            # Tool for performing exhaustive search over specified hyperparameter values using cross-validation\n",
    "\n",
    "from sklearn.pipeline import Pipeline                       # Used to create modeling pipelines\n",
    "from sklearn.preprocessing import StandardScaler            # Used for feature scaling (standardization)\n",
    "\n",
    "import joblib                                               # Import the joblib library to save and load the trained model efficiently\n",
    "\n",
    "# Set visualization style to dark grid for better readability\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Configure pandas display options to show full data without truncation\n",
    "pd.set_option(\"display.max_columns\", None)                  # Show all columns in outputs\n",
    "pd.set_option(\"display.max_colwidth\", None)                 # Show full content of each column (no '...')\n",
    "\n",
    "# Suppress FutureWarning messages to keep the output clean\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "np.random.seed(42)                                          # Set random seed for reproducibility (ensures same random results every time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd60492",
   "metadata": {},
   "source": [
    "# üì•üîç Loading and Discovering the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd6a23",
   "metadata": {},
   "source": [
    "- loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file into a DataFrame\n",
    "df = pd.read_csv('Airline_Passenger_Satifaction_Data.csv')  \n",
    "print('Loading is Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee82163",
   "metadata": {},
   "source": [
    "- Showing some details about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a tuple with the number of rows and columns in the DataFrame (rows, columns)\n",
    "df.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the first 5 rows of the DataFrame to get a quick overview of the data\n",
    "df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays summary info about the DataFrame: columns, non-null values, and data types\n",
    "df.info()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee9308",
   "metadata": {},
   "source": [
    "- Exploring unknown values ‚Äã‚Äãin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the number of missing (NaN) values in each column of the DataFrame\n",
    "df.isna().sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all rows that contain at least one NaN value\n",
    "df[df.isna().any(axis=1)].head(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5798ca",
   "metadata": {},
   "source": [
    "- Exploring duplicated rows ‚Äã‚Äãin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if there are any duplicated rows in the DataFrame and returns True or False\n",
    "print(df.duplicated().any())\n",
    "\n",
    "# Count duplicated rows\n",
    "print(df.duplicated().sum())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1aef5d",
   "metadata": {},
   "source": [
    "- Exploring illogical values ‚Äã‚Äãin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display columns that contain at least one negative value (numeric only)\n",
    "negative_columns = df.select_dtypes(include=[np.number]).columns[(df.select_dtypes(include=[np.number]) < 0).any()]\n",
    "print(\"Columns with negative values:\", list(negative_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac656c",
   "metadata": {},
   "source": [
    "- Display summary statistics for all object columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629dd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for categorical (object type) columns\n",
    "df.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f51b8e",
   "metadata": {},
   "source": [
    "### ‚úÖ After using these functions we noticed many things:\n",
    "\n",
    "- Data loaded successfully.\n",
    "- Number of Rows is: 103988.\n",
    "- Number of columns is: 25.\n",
    "- There are null values in 21 columns (359 NaN value in data).\n",
    "- There is 66 complete duplicates.\n",
    "- There is a column with negative values [Age].\n",
    "- Unnamed:0 and id columns wouldn't be useful for our project.\n",
    "- There is a problem with the data type in the 'online boarding' column (it is supposed to be numeric)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef51f0",
   "metadata": {},
   "source": [
    "# üìàüßπ Data preparation and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4180e",
   "metadata": {},
   "source": [
    "At this stage we will get rid of unimportant columns and process all problems of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a313580",
   "metadata": {},
   "source": [
    "- Delete unimportant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ace2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns with specific names\n",
    "df = df.drop(columns=['Unnamed: 0', 'id']) \n",
    "\n",
    "# To verify, print the column names after deletion\n",
    "print(df.columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d96ea",
   "metadata": {},
   "source": [
    "- Remove duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6903d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Verify the result by checking the shape \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea9513",
   "metadata": {},
   "source": [
    "- Process some unknown and fales values ‚Äã‚Äã(Imputation)\n",
    "\n",
    "Replace unknown, negative, or very large values ‚Äã‚Äãwith the average values ‚Äã‚Äãin the 'Age' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of valid ages (between 0 and 100)\n",
    "valid_age_mean = df.loc[(df['Age'] > 0) & (df['Age'] < 100), 'Age'].mean()\n",
    "\n",
    "# Replace ages less than 0 or greater than 100 with the valid mean\n",
    "df.loc[(df['Age'] < 0) | (df['Age'] > 100), 'Age'] = valid_age_mean\n",
    "\n",
    "# Replace NaN values with the mean\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b909f",
   "metadata": {},
   "source": [
    "- Drop rows of unknown values\n",
    "\n",
    "Dropping unknown values ‚Äã‚Äãis a good option because their number is small compared to the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79476875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that contain any NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# This will show the number of rows and columns after removal\n",
    "print(df.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2efd76",
   "metadata": {},
   "source": [
    "- Ensuring that unknown values ‚Äã‚Äãare deleted or processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the number of missing (NaN) values in each column of the DataFrame\n",
    "df.isna().sum()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf2b40",
   "metadata": {},
   "source": [
    "> Great! Our Data without any NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b628a53b",
   "metadata": {},
   "source": [
    "- discovering the type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7989bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each column in the DataFrame\n",
    "for col in df.columns:\n",
    "    # Get the unique data types (after dropping NaNs)\n",
    "    types = df[col].dropna().apply(type).unique()\n",
    "    \n",
    "    # If the column has only one type\n",
    "    if len(types) == 1:\n",
    "        # Check if it's numeric (int or float)\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            print(f\"üî¢ Numeric: {col}\")\n",
    "        else:\n",
    "            print(f\"üî§ Categorical: {col}\")\n",
    "    else:\n",
    "        # Column contains mixed data types (e.g., int and str)\n",
    "        print(f\"‚ö†Ô∏è Mixed-type: {col} ({types})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274dc80",
   "metadata": {},
   "source": [
    "> Oops! there is a problem with values in 'Online boarding' column. the values are mixed-type !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c308a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the frequency of each unique value in the 'Online boarding' column, including NaNs if any\n",
    "print(df['Online boarding'].value_counts(dropna=False))\n",
    "\n",
    "# Print the count of different data types present in the 'Online boarding' column\n",
    "# This helps identify if the column contains mixed types (e.g., int and str), which can cause encoding issues\n",
    "print(df['Online boarding'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae193621",
   "metadata": {},
   "source": [
    "> To solve this problem we neet to cast all values of column to integer data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all values in the 'Online boarding' column to integers\n",
    "# Invalid parsing will be set as NaN (using 'errors=\"coerce\"'), and we use 'Int64' to allow nullable integers\n",
    "df['Online boarding'] = pd.to_numeric(df['Online boarding'], errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11419728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any missing (NaN) values in the 'Online boarding' column after conversion\n",
    "df['Online boarding'].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ac735",
   "metadata": {},
   "source": [
    "- check dtype for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to know the number of Categorical columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a2749",
   "metadata": {},
   "source": [
    "### ‚úÖ Now, we can say that our data is almost ready\n",
    "\n",
    "- Unnecessary columns were removed.\n",
    "- Duplicate rows were removed.\n",
    "- Some unknown values ‚Äã‚Äãwere processed and some were deleted.\n",
    "- The data type of the columns was checked and the mixed-dtype issue was solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c632cf",
   "metadata": {},
   "source": [
    "# üì∂üßê Data Visualization and Exploration (to Gain Insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09957fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next 5 lines define the default font sizes\n",
    "plt.rc('font', size=10)\n",
    "plt.rc('axes', labelsize=14, titlesize=18)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c92b2a",
   "metadata": {},
   "source": [
    "- Plot a Histogram for Each Column in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate histograms for all numerical columns in the DataFrame\n",
    "df.hist(bins=22, figsize=(20, 18), grid = True)\n",
    "\n",
    "# Show the histogram plot on the screen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024c4b3",
   "metadata": {},
   "source": [
    "- Age Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing on Age, we'll analyze the age distribution using a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Age'], kde=True, bins=30, color='blue')\n",
    "plt.title('Age Distribution of Passengers')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# This code shows the distribution of ages in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fdde6d",
   "metadata": {},
   "source": [
    "- Satisfaction Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b2011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we will analyze the satisfaction distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df, x='satisfaction', palette=\"Set2\")\n",
    "plt.title('Satisfaction Distribution')\n",
    "plt.xlabel('Satisfaction (0: Neutral/Dissatisfied, 1: Satisfied)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "# This plot shows the count of passengers who are satisfied (1) versus those who are neutral or dissatisfied (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d12ad",
   "metadata": {},
   "source": [
    "- Departure Delay Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we will visualize the departure delay distribution \n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['Departure Delay in Minutes'], kde=True, bins=30, color='green')\n",
    "plt.title('Departure Delay Distribution')\n",
    "plt.xlabel('Departure Delay in Minutes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# This analysis helps us understand the distribution of delays and how frequent they are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8089d5c",
   "metadata": {},
   "source": [
    "- Correlation Heatmap (For Numeric Columns Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdacbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns and compute correlation matrix\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Now, calculate and plot the correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.show()\n",
    "# This version of the heatmap will only show correlations between numerical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f90c47",
   "metadata": {},
   "source": [
    "> The heatmap shows a strong correlation between 'Departure Delay in Minutes' & 'Arrival Delay in Minutes', let's go to dive into it to discover more about this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the font size for x and y axis tick labels to make the plot more readable\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "# Create a scatter plot to visualize the relationship between Arrival Delay and Departure Delay\n",
    "sns.scatterplot(x='Arrival Delay in Minutes', y='Departure Delay in Minutes', data=df)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74857d09",
   "metadata": {},
   "source": [
    "> We can notice that Departure Delay is approximately equal to Arrival Delay. That tells us that the delay happens only before the start of the flight (not during the flight), it required feature engineering step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the flight delay by subtracting Departure Delay from Arrival Delay\n",
    "df['Flight Delay'] = df['Arrival Delay in Minutes'] - df['Departure Delay in Minutes']\n",
    "\n",
    "# Show the mean of the new column\n",
    "print(df['Flight Delay'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95471e80",
   "metadata": {},
   "source": [
    "> Since the difference between 'Departure Delay' and 'Arrival Delay' is minimal (~0.43 minutes on average). we can safely remove one of them to avoid redundancy. We'll keep 'Departure Delay' and drop 'Arrival Delay'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Arrival Delay in Minutes' and the engineered 'Flight Delay' columns \n",
    "# because they are highly correlated with 'Departure Delay in Minutes' \n",
    "# and keeping all of them would cause multicollinearity in the model.\n",
    "df.drop(columns=['Arrival Delay in Minutes', 'Flight Delay'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425ccff",
   "metadata": {},
   "source": [
    "- Missing Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will visualize the missing values in the dataset using a heatmap.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isna(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data Heatmap')\n",
    "plt.show()\n",
    "# This heatmap highlights the missing (NaN) values in the dataset and allows us to check if any columns are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays summary info about the DataFrame: columns, non-null values, and data types\n",
    "df.info()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446ac4a",
   "metadata": {},
   "source": [
    "# ü™ìüìä Data splitting (into train set and test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc52cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'satisfaction' column from the DataFrame (X will be all the features except 'satisfaction')\n",
    "X = df.drop(\"satisfaction\", axis=1)\n",
    "\n",
    "# Select the 'satisfaction' column as the target variable (y is the target we want to predict)\n",
    "y = df[\"satisfaction\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# 80% of the data will be used for training, and 20% for testing\n",
    "# The random_state ensures reproducibility (same split every time)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Show the number of samples in the training set (X_train), indicating how many training data points we have.\n",
    "print(f\"The number of samples in the training set is: {len(X_train)}\")\n",
    "\n",
    "# Show the number of samples in the test set (X_test), showing how many data points are in the test set.\n",
    "print(f\"The number of samples in the test set is: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4284d6",
   "metadata": {},
   "source": [
    "# üî°üî¢ Data encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce6467",
   "metadata": {},
   "source": [
    "- Encoding X (Features) with Mapping Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store label encoders for each column\n",
    "feature_encoders = {}\n",
    "\n",
    "# Loop through object columns in X_train to fit and transform\n",
    "for column in X_train.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train[column] = le.fit_transform(X_train[column])\n",
    "    X_test[column] = le.transform(X_test[column])  \n",
    "    feature_encoders[column] = le\n",
    "\n",
    "    # Print mapping for this feature\n",
    "    print(f\"\\nMapping for feature column: {column}\")\n",
    "    for i, class_ in enumerate(le.classes_):\n",
    "        print(f\"{class_} --> {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78203fc",
   "metadata": {},
   "source": [
    "- Encoding y (Target) with Mapping Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable using LabelEncoder\n",
    "target_encoder = LabelEncoder()\n",
    "y_train = target_encoder.fit_transform(y_train)\n",
    "y_test = target_encoder.transform(y_test)\n",
    "\n",
    "# Print mapping for the target\n",
    "print(\"\\nMapping for target:\")\n",
    "for i, class_ in enumerate(target_encoder.classes_):\n",
    "    print(f\"{class_} --> {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e98a0",
   "metadata": {},
   "source": [
    "- Casting All Numeric Columns to Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fac9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all values in the training and test sets (features and target) to integers\n",
    "def cast_all_to_int(df):\n",
    "    for column in df.select_dtypes(include=['float', 'int']).columns:\n",
    "        df[column] = df[column].astype(int)\n",
    "    return df\n",
    "\n",
    "X_train = cast_all_to_int(X_train)                  # Cast all feature columns in X_train to integers\n",
    "y_train = cast_all_to_int(pd.DataFrame(y_train))    # Cast the target column in y_train to integers\n",
    "X_test = cast_all_to_int(X_test)                    # Cast all feature columns in X_test to integers\n",
    "y_test = cast_all_to_int(pd.DataFrame(y_test))      # Cast the target column in y_test to integers\n",
    "\n",
    "print(X_train.info())\n",
    "print(y_train.info())\n",
    "print(X_test.info())\n",
    "print(y_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7512b",
   "metadata": {},
   "source": [
    "# üõ†Ô∏èüìà ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2309b",
   "metadata": {},
   "source": [
    "- Define the Models Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c87463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary to store various machine learning models with their names as keys\n",
    "models = {\n",
    "    # Logistic Regression model\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        ('scaler', StandardScaler()),                   # Step to standardize the features before fitting the model\n",
    "        ('logreg', LogisticRegression(max_iter=10000))  # Logistic Regression model with sufficient iterations for convergence\n",
    "    ]),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),          # Decision Tree Classifier\n",
    "    \"Random Forest\": RandomForestClassifier(),          # Random Forest Classifier\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),  # Gradient Boosting Classifier\n",
    "    # K-Nearest Neighbors Classifier\n",
    "    \"KNN\": Pipeline([\n",
    "        ('scaler', StandardScaler()),                   # Step to standardize the features before fitting the KNN model\n",
    "        ('knn', KNeighborsClassifier())                 # KNN model (which benefits from scaling for distance calculations)\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151e86a",
   "metadata": {},
   "source": [
    "- Train and evaluate each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Variables to Track Best Model\n",
    "best_model = None\n",
    "best_score = 0\n",
    "best_model_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"\\nüìå {name}\")\n",
    "\n",
    "    # Convert y_train and y_test to 1D arrays to avoid warnings\n",
    "    y_train = y_train.ravel()  # Flatten y_train\n",
    "    y_test = y_test.ravel()    # Flatten y_test\n",
    "    \n",
    "    # Start measuring time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform 5-fold Cross-Validation on training data\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    mean_cv = cv_scores.mean()\n",
    "    print(f\"‚úÖ Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"‚úÖ Mean CV score: {mean_cv:.4f}\")\n",
    "    \n",
    "    # Fit the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print Accuracy and Classification Report\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"üéØ Accuracy on Test Set: {acc:.4f}\")\n",
    "    print(\"üìã Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Display Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate time taken\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"‚è±Ô∏è Time taken: {duration:.2f} seconds\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Track best model based on highest mean CV score\n",
    "    if mean_cv > best_score:\n",
    "        best_score = mean_cv\n",
    "        best_model = model\n",
    "        best_model_name = name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642ff7f",
   "metadata": {},
   "source": [
    "# üìù the Best Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5edd121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Print the Best Model Summary\n",
    "print(\"\\n Best Model Selected:\")\n",
    "print(f\" Model: {best_model_name}\")\n",
    "print(f\" Best Mean CV Score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f0ea7",
   "metadata": {},
   "source": [
    "# üîç Hyperparameter Tuning for Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64205d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],        # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20],            # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5],            # Minimum number of samples required to split a node\n",
    "    'max_features': ['sqrt', 'log2']        # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Create a base Random Forest model\n",
    "rf_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search with 5-fold Cross-Validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,            # Use all available CPU cores\n",
    "    verbose=1             # Show progress\n",
    ")\n",
    "\n",
    "# Fit Grid Search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ‚úÖ Display the best parameters and best score\n",
    "print(\"\\n‚úÖ‚úÖ Best Parameters Found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"üìà Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "'''\n",
    "‚úÖ‚úÖ Best Parameters Found:\n",
    "{'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 300}\n",
    "üìà Best Cross-Validation Accuracy: 0.9619\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8bbd3",
   "metadata": {},
   "source": [
    "# üìù Evaluate the Tuned Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator after tuning\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_tuned = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(f\"\\nüéØ Accuracy on Test Set after Tuning: {accuracy_score(y_test, y_pred_tuned):.4f}\")\n",
    "print(\"üìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "# Display Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title('Confusion Matrix - Tuned Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1307c5d",
   "metadata": {},
   "source": [
    "# üíæ‚úÖ Saving the Trained Model with Joblib for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e18274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(best_model, 'random_forest_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
